{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4bfa90-5bfa-45bf-8049-ce67a40b2bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "random seed: 1234\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Enable tqdm in pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Select device (GPU if available)\n",
    "use_gpu = True\n",
    "device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')\n",
    "print(f'device: {device.type}')\n",
    "\n",
    "# Set random seed\n",
    "seed = 1234\n",
    "if seed is not None:\n",
    "    print(f'random seed: {seed}')\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a724ec-8b43-4f6a-9eae-899abc3cc95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Transformer Model and Tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "print(f\"Loading model: {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632ced98-1890-42a0-a32d-3d6dbbce77a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset from assignment4-dataset.txt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb0b2038f784b76bcab5bf0854380e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing Token Embeddings:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing final averages...\n",
      "Computed embeddings for 39833 unique tokens.\n",
      "Save complete! Embeddings saved to token_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "# Data structures to store sums and counts\n",
    "token_sums = defaultdict(lambda: np.zeros(model.config.hidden_size))\n",
    "token_counts = defaultdict(int)\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = 'assignment4-dataset.txt' \n",
    "\n",
    "print(f\"Processing dataset from {dataset_path}...\")\n",
    "\n",
    "# Set the limit to 1 million lines\n",
    "LIMIT_LINES = 100_000\n",
    "\n",
    "# Open file and process\n",
    "with open(dataset_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    # loop with progress bar\n",
    "    for i, line in tqdm(enumerate(f), total=LIMIT_LINES, desc=\"Computing Token Embeddings\"):\n",
    "        \n",
    "        # Stop after 1 million lines\n",
    "        if i >= LIMIT_LINES:\n",
    "            break\n",
    "            \n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = tokenizer(line, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get Contextual Embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Shape: (1, seq_len, hidden_size)\n",
    "            embeddings = outputs.last_hidden_state.squeeze(0).cpu().numpy()\n",
    "            \n",
    "        # Map IDs to Embeddings\n",
    "        input_ids = inputs['input_ids'].squeeze(0).cpu().numpy()\n",
    "        \n",
    "        # Accumulate\n",
    "        for token_id, vector in zip(input_ids, embeddings):\n",
    "            token_sums[token_id] += vector\n",
    "            token_counts[token_id] += 1\n",
    "\n",
    "# --- AVERAGING & SAVING ---\n",
    "print(\"Computing final averages...\")\n",
    "static_token_embeddings = {}\n",
    "for token_id, total_vector in token_sums.items():\n",
    "    static_token_embeddings[token_id] = total_vector / token_counts[token_id]\n",
    "\n",
    "print(f\"Computed embeddings for {len(static_token_embeddings)} unique tokens.\")\n",
    "\n",
    "# Save to file\n",
    "save_path = 'token_embeddings.pt'\n",
    "torch.save(static_token_embeddings, save_path)\n",
    "print(f\"Save complete! Embeddings saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cd21ac-bf54-499c-851b-37aadb90a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building word embeddings from vocabulary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b50f447e014da2b2062e5d9f0a3596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Vocab: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed embeddings for 400000 words.\n"
     ]
    }
   ],
   "source": [
    "def get_word_embedding(word, tokenizer, token_embeddings):\n",
    "    \"\"\"\n",
    "    Tokenizes a word and averages the static embeddings of its sub-word tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the word (add_special_tokens=False to avoid <s> and </s> wrapper tokens)\n",
    "    token_ids = tokenizer.encode(word, add_special_tokens=False)\n",
    "    \n",
    "    vectors = []\n",
    "    for tid in token_ids:\n",
    "        # Only use vectors if we actually saw this token in the large dataset\n",
    "        if tid in token_embeddings:\n",
    "            vectors.append(token_embeddings[tid])\n",
    "            \n",
    "    if not vectors:\n",
    "        # If the word consists entirely of tokens we never saw in the dataset\n",
    "        return np.zeros(768) \n",
    "        \n",
    "    # Average the sub-word vectors to get the whole word vector\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Path to vocabulary file\n",
    "vocab_path = 'glove.6B.300d-vocabulary.txt'\n",
    "word_embeddings = {}\n",
    "\n",
    "print(\"Building word embeddings from vocabulary...\")\n",
    "\n",
    "# Open as standard text file\n",
    "with open(vocab_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in tqdm(f, desc=\"Processing Vocab\"):\n",
    "        word = line.strip()\n",
    "        if not word: continue\n",
    "        \n",
    "        word_embeddings[word] = get_word_embedding(word, tokenizer, static_token_embeddings)\n",
    "\n",
    "print(f\"Computed embeddings for {len(word_embeddings)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf396bdd-6452-48ad-aeed-3f9e5efd85ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words to 'apple':\n",
      "  applecart: 1.0000\n",
      "  applewood: 0.9745\n",
      "  applejack: 0.9705\n",
      "  appleby: 0.9697\n",
      "  applebaum: 0.9695\n",
      "------------------------------\n",
      "Most similar words to 'king':\n",
      "  kingdoms: 1.0000\n",
      "  kingman: 0.9874\n",
      "  kingma: 0.9869\n",
      "  kingston: 0.9863\n",
      "  peking: 0.9857\n",
      "------------------------------\n",
      "Most similar words to 'happy':\n",
      "  unhappy: 0.9631\n",
      "  happyland: 0.9628\n",
      "  happyness: 0.9626\n",
      "  trigger-happy: 0.9580\n",
      "  witgood: 0.9207\n",
      "------------------------------\n",
      "Most similar words to 'car':\n",
      "  microcar: 1.0000\n",
      "  carloads: 1.0000\n",
      "  truecar: 1.0000\n",
      "  carreon: 1.0000\n",
      "  carousel: 1.0000\n",
      "------------------------------\n",
      "Most similar words to 'university':\n",
      "  interuniversity: 0.9931\n",
      "  inter-university: 0.9875\n",
      "  university-wide: 0.9871\n",
      "  university-educated: 0.9870\n",
      "  university-level: 0.9867\n",
      "------------------------------\n",
      "Most similar words to 'science':\n",
      "  materialscience: 1.0000\n",
      "  neuroscience: 0.9625\n",
      "  e-science: 0.9546\n",
      "  science-fiction: 0.9520\n",
      "  science-related: 0.9515\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def most_similar(word, k=5):\n",
    "    \"\"\"\n",
    "    Finds the k most similar words to the input word using Cosine Similarity.\n",
    "    \"\"\"\n",
    "    if word not in word_embeddings:\n",
    "        print(f\"Word '{word}' not found in vocabulary.\")\n",
    "        return\n",
    "    \n",
    "    # Get the vector for the target word\n",
    "    target_vector = word_embeddings[word].reshape(1, -1)\n",
    "    \n",
    "    # Prepare matrix of all vocab vectors for fast calculation\n",
    "    vocab_words = list(word_embeddings.keys())\n",
    "    vocab_vectors = np.array([word_embeddings[w] for w in vocab_words])\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    sim_scores = cosine_similarity(target_vector, vocab_vectors)[0]\n",
    "    \n",
    "    # Get top k indices (sorting descending)\n",
    "    # We grab k+1 because the most similar word is always the word itself (score 1.0)\n",
    "    top_indices = sim_scores.argsort()[-(k+1):][::-1]\n",
    "    \n",
    "    print(f\"Most similar words to '{word}':\")\n",
    "    count = 0\n",
    "    for idx in top_indices:\n",
    "        sim_word = vocab_words[idx]\n",
    "        \n",
    "        # Skip the word itself\n",
    "        if sim_word == word: \n",
    "            continue \n",
    "            \n",
    "        score = sim_scores[idx]\n",
    "        print(f\"  {sim_word}: {score:.4f}\")\n",
    "        \n",
    "        count += 1\n",
    "        if count >= k:\n",
    "            break\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Run Examples ---\n",
    "# These are standard test words, but verify if your assignment PDF \n",
    "# specified different ones in a separate area not visible in the screenshot.\n",
    "test_words = [\"apple\", \"king\", \"happy\", \"car\", \"university\", \"science\"]\n",
    "\n",
    "for w in test_words:\n",
    "    most_similar(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f852e7-4e71-44b8-a391-213abcf10528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
